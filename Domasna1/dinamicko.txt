# Required Libraries
!apt-get update
!apt-get install -y chromium-browser
!apt-get install -y chromium-chromedriver

import os
import time
import csv
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import requests

# --- Setup Chrome WebDriver for Google Colab ---
chrome_driver_path = '/usr/lib/chromium-browser/chromedriver'

# Setup Chrome options for headless browsing in Google Colab
chrome_options = Options()
chrome_options.add_argument('--headless')  # Run in headless mode (no GUI)
chrome_options.add_argument('--no-sandbox')  # Necessary for Colab
chrome_options.add_argument('--disable-dev-shm-usage')  # Required for Colab to avoid crashes
chrome_options.add_argument('--remote-debugging-port=9222')  # Debugging port

# Add the Chromium and ChromeDriver path to environment
os.environ["PATH"] += ":/usr/lib/chromium-browser/chromedriver"

# Function to create and return a WebDriver instance
def create_driver():
    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=chrome_options)
    return driver

# --- Get Dynamic Date Range for the Last 10 Years ---
def get_dynamic_date_range(years_ago):
    current_date = datetime.now()
    end_date = current_date
    start_date = end_date - timedelta(days=365)

    date_ranges = []
    for _ in range(years_ago):
        date_ranges.append((start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')))
        end_date = start_date - timedelta(days=1)
        start_date = end_date - timedelta(days=365)

    return date_ranges

# --- Scrape Data for a Specific Date Range ---
def scrape_data_for_date_range(driver, option_value, base_url, from_date, to_date):
    option_url = f"{base_url}{option_value}"
    driver.get(option_url)

    # Wait for the page to load
    time.sleep(3)  # Adjust timing based on the page load speed

    # Find the "From" and "To" dropdowns
    from_dropdown = driver.find_element(By.NAME, 'from_date')  # Use actual name or selector
    to_dropdown = driver.find_element(By.NAME, 'to_date')  # Use actual name or selector

    # Select the correct "From" and "To" dates dynamically
    from_dropdown.send_keys(from_date)
    to_dropdown.send_keys(to_date)

    # Click the "Find" button to submit the form
    find_button = driver.find_element(By.NAME, 'find_button')  # Use actual button name or selector
    find_button.click()

    # Wait for the page to load after submitting
    time.sleep(5)

    # Parse the page content with BeautifulSoup
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    table = soup.find('table')  # Locate the table on the page

    if table:
        rows = table.find_all('tr')
        table_data = []
        table_headers = []

        for idx, row in enumerate(rows):
            columns = row.find_all('td')
            if columns:
                if idx == 0:  # Header row
                    table_headers = [col.text.strip() for col in columns]
                else:  # Data rows
                    table_data.append([col.text.strip() for col in columns])

        # Print the data for the current date range
        print(f"Headers: {table_headers}")
        print(f"Data for {option_value} from {from_date} to {to_date}:")
        for row in table_data:
            print(row)

        return {"option_value": option_value, "headers": table_headers, "data": table_data}
    else:
        print(f"No table found for {from_date} to {to_date}")
        return {"option_value": option_value, "headers": [], "data": []}

# --- Scrape and Merge Data for Ten Years ---
def scrape_and_merge_data_for_ten_years(driver, option_value, base_url):
    all_data = []
    all_headers = []

    # Get the dynamic date ranges for 10 years
    date_ranges = get_dynamic_date_range(10)  # Scrape for 10 years

    # Scrape for each date range (from, to)
    for from_date, to_date in date_ranges:
        data_for_year = scrape_data_for_date_range(driver, option_value, base_url, from_date, to_date)

        if data_for_year['data']:
            all_data.extend(data_for_year['data'])
            if not all_headers and data_for_year['headers']:
                all_headers = data_for_year['headers']

    # Deduplicate the data
    deduplicated_data = []
    seen_rows = set()

    for row in all_data:
        row_tuple = tuple(row)
        if row_tuple not in seen_rows:
            seen_rows.add(row_tuple)
            deduplicated_data.append(row)

    return {"option_value": option_value, "headers": all_headers, "data": deduplicated_data}

# --- Filter Dropdown Options ---
def filter_1(url, csv_filename):
    # Use requests to get the page (for dropdown)
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to retrieve dropdown options. Status code: {response.status_code}")
        return []

    # Parse the page content with BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the dropdown (assuming it is a <select> element)
    dropdown = soup.find('select')
    if not dropdown:
        print(f"No dropdown found on the page.")
        return []

    options = dropdown.find_all('option')
    # Filter options without numbers in their value
    filtered_options = [{'text': option.text.strip(), 'value': option.get('value')} for option in options if not any(char.isdigit() for char in option.get('value'))]

    # Save filtered options to CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Option Text", "Option Value"])
        for option in filtered_options:
            writer.writerow([option['text'], option['value']])

    print(f"Filtered options saved to '{csv_filename}'")
    return filtered_options

# --- Full Scraping Pipeline ---
def run_pipeline(url, csv_filename, base_url, output_filename):
    # Initialize WebDriver
    driver = create_driver()

    # Step 1: Filter dropdown options and save them to CSV
    print("Running filter 1...")
    filtered_options = filter_1(url, csv_filename)

    # Step 2: Scrape data for each option for 10 years
    print("\nRunning filter 2 (Scraping)...")
    scraped_data = []

    for option in filtered_options:
        data = scrape_and_merge_data_for_ten_years(driver, option['value'], base_url)
        if data['data']:
            scraped_data.append(data)

    # Step 3: Save the scraped data to a CSV file
    if output_filename:
        print(f"\nSaving scraped data to {output_filename}...")
        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            for option_data in scraped_data:
                writer.writerow(option_data['headers'])
                for row in option_data['data']:
                    writer.writerow(row)
        print(f"Scraped data saved to '{output_filename}'")

    # Clean up and quit the driver
    driver.quit()

# Example usage
url = 'https://www.mse.mk/en/stats/symbolhistory/FROT'
csv_filename = 'filtered_options.csv'
base_url = 'https://www.mse.mk/en/stats/symbolhistory/'
output_filename = 'merged_scraped_data_10_years.csv'

start_time = time.time()
run_pipeline(url, csv_filename, base_url, output_filename)
end_time = time.time()

print(f"Scraping completed in {end_time - start_time} seconds.")
