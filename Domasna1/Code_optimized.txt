!pip install selenium
!pip install webdriver-manager
!apt-get update # Update package list
!apt-get install -y wget curl unzip
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt --fix-broken install -y
!google-chrome-stable --version # Check installed version

import csv
import os
import re
import glob
import pandas as pd
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Set up Selenium WebDriver options for headless browsing (without opening a window)
options = Options()
options.add_argument("--headless")  # Run in headless mode (no GUI)
options.add_argument("--no-sandbox")  # Add this flag if you are running in an environment without a display
options.add_argument("--disable-dev-shm-usage")  # Overcome limited shared memory issues
options.add_argument("--remote-debugging-port=9222")  # Enable remote debugging (may help resolve errors)
options.add_argument("--disable-gpu")  # Disable GPU acceleration in headless mode
options.add_argument("--start-maximized")  # Start Chrome maximized (optional, sometimes helps)
options.add_argument('--disable-software-rasterizer')  # Disable software rasterizer to avoid GPU-related issues

# Set up WebDriver using webdriver_manager to handle ChromeDriver installation
service = Service(ChromeDriverManager().install())

# Initialize the Selenium WebDriver with the options and service
driver = webdriver.Chrome(service=service, options=options)
driver.set_page_load_timeout(10)

def get_date_ranges():
    """Generate date ranges for 1 year ago, 2 years ago, ..., 10 years ago."""
    today = datetime.today()

    # Define the date ranges
    date_ranges = []
    for i in range(1, 11):  # Generate 1-year, 2-year, ..., 10-year ranges
        start_date = today - timedelta(days=365 * i)
        end_date = today - timedelta(days=365 * (i - 1))
        date_ranges.append((start_date.strftime('%m/%d/%y'), end_date.strftime('%m/%d/%y')))
    
    return date_ranges

def update_historical_data(symbol, row_data, headers):
    """Update the historical data CSV for the symbol."""
    filename = f"historical_data_{symbol}.csv"
    
    # Check if the file exists
    if os.path.exists(filename):
        try:
            existing_data = pd.read_csv(filename, dtype=str)  # Read as strings
            last_date = existing_data['Date'].max()  # Get the max date as string
        except pd.errors.EmptyDataError:
            # If file is empty, start a new dataframe
            existing_data = pd.DataFrame(columns=headers)
            last_date = None
    else:
        # If file doesn't exist, create a new empty dataframe
        existing_data = pd.DataFrame(columns=headers)
        last_date = None
    
    # Append the new row without modifying existing data
    new_row = pd.DataFrame([row_data], columns=headers)
    merged_data = pd.concat([existing_data, new_row]).drop_duplicates(subset=['Date']).sort_values(by='Date')
    merged_data.to_csv(filename, index=False)

# Go to the main page with the issuer symbols
url = 'https://www.mse.mk/en/stats/symbolhistory/TLE'
driver.get(url)

# Wait for the dropdown to be visible (instead of time.sleep)
wait = WebDriverWait(driver, 20)
dropdown = wait.until(EC.presence_of_element_located((By.ID, 'Code')))

# Scrape all the options from the dropdown
symbols = []
options = dropdown.find_elements(By.TAG_NAME, 'option')
for option in options:
    symbol = option.get_attribute('value')
    if symbol:
        symbols.append(symbol)

# Print out all the symbols found before filtering
print("All symbols:", symbols)

# Filter out symbols that contain numbers or any characters that are not letters or spaces
filtered_symbols = [symbol for symbol in symbols if re.match(r'^[A-Za-z\s]+$', symbol)]

# Print out the filtered symbols
print("Filtered symbols:", filtered_symbols)

# Get the 10 date ranges (1 year ago, 2 years ago, ..., 10 years ago)
date_ranges = get_date_ranges()

# Now, loop through each symbol and scrape the historical data page for it
for symbol in filtered_symbols:
    filename = f"historical_data_{symbol}.csv"
    
    # Loop through each of the 10 date ranges
    for start_date, end_date in date_ranges:
        print(f"Scraping data for {symbol} from {start_date} to {end_date}")

        # Build the URL dynamically for each symbol
        data_url = f'https://www.mse.mk/en/stats/symbolhistory/{symbol}'
        driver.get(data_url)

        # Wait for the "From Date" and "To Date" input fields to be present
        from_date_input = wait.until(EC.presence_of_element_located((By.ID, 'FromDate')))
        to_date_input = wait.until(EC.presence_of_element_located((By.ID, 'ToDate')))

        # Clear the current value and set the new date range
        from_date_input.clear()
        from_date_input.send_keys(start_date)
        to_date_input.clear()
        to_date_input.send_keys(end_date)

        # Wait for the "Submit" or "Search" button to be clickable before clicking it
        search_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'btn-primary-sm')))
        search_button.click()

        try:
            # Wait for the results table to appear after the page reloads
            results_table = wait.until(EC.presence_of_element_located((By.ID, 'resultsTable_wrapper')))
        except:
            # If the results table is not found, check for the "no-results" div
            no_results_div = driver.find_elements(By.CSS_SELECTOR, 'div.row.no-results')
            if no_results_div:
                print(f"No results found for {symbol} between {start_date} and {end_date}. Skipping this symbol.")
                continue  # Skip this symbol and move to the next one

        # Get the page source after JavaScript has been executed
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Find the table containing historical data
        table = soup.find('div', {'id': 'resultsTable_wrapper'})
        
        if table:
            headers = ['Date', 'Close Price', 'Max', 'Min', 'Avg Price', '% Change', 'Volume', 'Turnover Best', 'Total Turnover']
            
            # Iterate over the rows and extract data
            for row in table.find_all('tr')[1:]:  # Skip the header row
                columns = row.find_all('td')
                if len(columns) == 9:  # Ensure there are exactly 9 columns
                    # Extract and map data from each column
                    date = columns[0].text.strip()  # Date is in the first column
                    close_price = columns[1].text.strip()  # Close Price in the second column
                    max_price = columns[2].text.strip()  # Max in the third column
                    min_price = columns[3].text.strip()  # Min in the fourth column
                    avg_price = columns[4].text.strip()  # Average Price in the fifth column
                    percent_change = columns[5].text.strip()  # % Change in the sixth column
                    volume = columns[6].text.strip()  # Volume in the seventh column
                    turnover_best = columns[7].text.strip()  # Turnover Best in the eighth column
                    total_turnover = columns[8].text.strip()  # Total Turnover in the last column

                    # Pack all data into a row for CSV
                    row_data = [date, close_price, max_price, min_price, avg_price, percent_change, volume, turnover_best, total_turnover]
                    
                    # Call the function to update the CSV file
                    update_historical_data(symbol, row_data, headers)
                    
                    # Print the extracted data for each row
                    print(f"{symbol} | {date} | {close_price} | {max_price} | {min_price} | {avg_price} | {percent_change} | {volume} | {turnover_best} | {total_turnover}")
        else:
            print(f"No historical data found for {symbol} between {start_date} and {end_date}")

def merge_csv_files(output_filename="combined_historical_data.csv"):
    all_files = glob.glob("historical_data_*.csv")  # Get all CSV files
    combined_data = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
    combined_data.to_csv(output_filename, index=False)
    print(f"All data has been merged into {output_filename}")

# Call the merge function after all data has been scraped and saved
merge_csv_files()

# Close the browser once done
driver.quit()


