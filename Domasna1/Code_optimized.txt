!pip install selenium
!pip install webdriver-manager
!apt-get update # Update package list
!apt-get install -y wget curl unzip
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt --fix-broken install -y
!google-chrome-stable --version # Check installed version

import csv
import os
import re
import glob
import pandas as pd
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Set up Selenium WebDriver options for headless browsing (without opening a window)
options = Options()
options.add_argument("--headless")  # Run in headless mode (no GUI)
options.add_argument("--no-sandbox")  # Add this flag if you are running in an environment without a display
options.add_argument("--disable-dev-shm-usage")  # Overcome limited shared memory issues
options.add_argument("--remote-debugging-port=9222")  # Enable remote debugging (may help resolve errors)
options.add_argument("--disable-gpu")  # Disable GPU acceleration in headless mode
options.add_argument("--start-maximized")  # Start Chrome maximized (optional, sometimes helps)
options.add_argument('--disable-software-rasterizer')  # Disable software rasterizer to avoid GPU-related issues

# Set up WebDriver using webdriver_manager to handle ChromeDriver installation
service = Service(ChromeDriverManager().install())

# Initialize the Selenium WebDriver with the options and service
driver = webdriver.Chrome(service=service, options=options)
driver.set_page_load_timeout(10)

def get_date_range(last_date=None):
    today = datetime.today()

    if last_date and pd.notnull(last_date):
        # Attempt parsing last_date with multiple formats
        try:
            last_date = datetime.strptime(last_date, '%m/%d/%y')
        except ValueError:
            try:
                last_date = datetime.strptime(last_date, '%m/%d/%Y')
            except ValueError:
                print(f"Warning: Unable to parse date '{last_date}' in recognized formats.")
                return None, None  # Returning None if date parsing fails

        # Add one day to the parsed last_date to get the start_date
        start_date = last_date + timedelta(days=1)
    else:
        # Default to 10 years ago if no last_date is provided
        start_date = today - timedelta(days=365 * 10)

    return start_date.strftime('%m/%d/%y'), today.strftime('%m/%d/%y')

def update_historical_data(symbol, row_data, headers):
    filename = f"historical_data_{symbol}.csv"
    
    # Check if the file exists
    if os.path.exists(filename):
        try:
            existing_data = pd.read_csv(filename, dtype=str)  # Read as strings
            last_date = existing_data['Date'].max()  # Get the max date as string
        except pd.errors.EmptyDataError:
            # If file is empty, start a new dataframe
            existing_data = pd.DataFrame(columns=headers)
            last_date = None
    else:
        # If file doesn't exist, create a new empty dataframe
        existing_data = pd.DataFrame(columns=headers)
        last_date = None
    
    # Append the new row without modifying existing data
    new_row = pd.DataFrame([row_data], columns=headers)
    merged_data = pd.concat([existing_data, new_row]).drop_duplicates(subset=['Date']).sort_values(by='Date')
    merged_data.to_csv(filename, index=False)

# Go to the main page with the issuer symbols
url = 'https://www.mse.mk/en/stats/symbolhistory/TLE'
driver.get(url)

# Wait for the dropdown to be visible (instead of time.sleep)
wait = WebDriverWait(driver, 20)
dropdown = wait.until(EC.presence_of_element_located((By.ID, 'Code')))

# Scrape all the options from the dropdown
symbols = []
options = dropdown.find_elements(By.TAG_NAME, 'option')
for option in options:
    symbol = option.get_attribute('value')
    if symbol:
        symbols.append(symbol)

# Print out all the symbols found before filtering
print("All symbols:", symbols)

# Filter out symbols that contain numbers or any characters that are not letters or spaces
filtered_symbols = [symbol for symbol in symbols if re.match(r'^[A-Za-z\s]+$', symbol)]

# Print out the filtered symbols
print("Filtered symbols:", filtered_symbols)

# Get the date range for the past 10 years
start_date, end_date = get_date_range()

# Now, loop through each symbol and scrape the historical data page for it
for symbol in filtered_symbols:
    filename = f"historical_data_{symbol}.csv"
    last_date = None

    # Check if existing data file for symbol
    if os.path.exists(filename):
        try:
            existing_data = pd.read_csv(filename, dtype=str)  # Read as strings
            last_date = existing_data['Date'].max()
        except pd.errors.EmptyDataError:
            # If file is empty, proceed without reading last date
            last_date = None

    start_date, end_date = get_date_range(last_date)

    # Build the URL dynamically for each symbol
    data_url = f'https://www.mse.mk/en/stats/symbolhistory/{symbol}'
    driver.get(data_url)

    # Wait for the "From Date" and "To Date" input fields to be present
    from_date_input = wait.until(EC.presence_of_element_located((By.ID, 'FromDate')))
    to_date_input = wait.until(EC.presence_of_element_located((By.ID, 'ToDate')))

    # Clear the current value and set the new date range
    from_date_input.clear()
    from_date_input.send_keys(start_date)
    to_date_input.clear()
    to_date_input.send_keys(end_date)

    # Wait for the "Submit" or "Search" button to be clickable before clicking it
    search_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'btn-primary-sm')))
    search_button.click()

    # Wait for the results table to appear after the page reloads
    results_table = wait.until(EC.presence_of_element_located((By.ID, 'resultsTable_wrapper')))

    # Get the page source after JavaScript has been executed
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Find the table containing historical data
    table = soup.find('div', {'id': 'resultsTable_wrapper'})
    
    if table:
        headers = ['Date', 'Close Price', 'Max', 'Min', 'Avg Price', '% Change', 'Volume', 'Turnover Best', 'Total Turnover']
        
        # Iterate over the rows and extract data
        for row in table.find_all('tr')[1:]:  # Skip the header row
            columns = row.find_all('td')
            if len(columns) == 9:  # Ensure there are exactly 9 columns
                # Extract and map data from each column
                date = columns[0].text.strip()  # Date is in the first column
                close_price = columns[1].text.strip()  # Close Price in the second column
                max_price = columns[2].text.strip()  # Max in the third column
                min_price = columns[3].text.strip()  # Min in the fourth column
                avg_price = columns[4].text.strip()  # Average Price in the fifth column
                percent_change = columns[5].text.strip()  # % Change in the sixth column
                volume = columns[6].text.strip()  # Volume in the seventh column
                turnover_best = columns[7].text.strip()  # Turnover Best in the eighth column
                total_turnover = columns[8].text.strip()  # Total Turnover in the last column

                # Pack all data into a row for CSV
                row_data = [date, close_price, max_price, min_price, avg_price, percent_change, volume, turnover_best, total_turnover]
                
                # Call the function to update the CSV file
                update_historical_data(symbol, row_data, headers)
                
                # Print the extracted data for each row
                print(f"{symbol} | {date} | {close_price} | {max_price} | {min_price} | {avg_price} | {percent_change} | {volume} | {turnover_best} | {total_turnover}")
    else:
        print(f"No historical data found for {symbol}")

def merge_csv_files(output_filename="combined_historical_data.csv"):
    all_files = glob.glob("historical_data_*.csv")  # Get all CSV files
    combined_data = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
    combined_data.to_csv(output_filename, index=False)
    print(f"All data has been merged into {output_filename}")

# Call the merge function after all data has been scraped and saved
merge_csv_files()

# Close the browser once done
driver.quit()

