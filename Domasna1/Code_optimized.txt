!pip install selenium
!pip install webdriver-manager
!apt-get update # Update package list
!apt-get install -y wget curl unzip
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt --fix-broken install -y
!google-chrome-stable --version # Check installed version

import csv
import os
import re
import glob
import pandas as pd
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Set up Selenium WebDriver options for headless browsing (without opening a window)
options = Options()
options.add_argument("--headless")  # Run in headless mode (no GUI)
options.add_argument("--no-sandbox")  # Add this flag if you are running in an environment without a display
options.add_argument("--disable-dev-shm-usage")  # Overcome limited shared memory issues
options.add_argument("--remote-debugging-port=9222")  # Enable remote debugging (may help resolve errors)
options.add_argument("--disable-gpu")  # Disable GPU acceleration in headless mode
options.add_argument("--start-maximized")  # Start Chrome maximized (optional, sometimes helps)
options.add_argument('--disable-software-rasterizer')  # Disable software rasterizer to avoid GPU-related issues

# Set up WebDriver using webdriver_manager to handle ChromeDriver installation
service = Service(ChromeDriverManager().install())

# Initialize the Selenium WebDriver with the options and service
driver = webdriver.Chrome(service=service, options=options)
driver.set_page_load_timeout(10)

def get_date_range(last_date=None):
    today = datetime.today()
    
    if last_date and pd.notnull(last_date):
        # Attempt parsing last_date with multiple formats
        try:
            last_date = datetime.strptime(last_date, '%m/%d/%y')
        except ValueError:
            try:
                last_date = datetime.strptime(last_date, '%m/%d/%Y')
            except ValueError:
                print(f"Warning: Unable to parse date '{last_date}' in recognized formats.")
                return None, None  # Returning None if date parsing fails

        # Add one day to the parsed last_date to get the start_date
        start_date = last_date + timedelta(days=1)
    else:
        # Default to 10 years ago if no last_date is provided
        start_date = today - timedelta(days=365 * 10)

    return start_date.strftime('%m/%d/%y'), today.strftime('%m/%d/%y')

def update_historical_data(symbol, date, close_price, volume):
    filename = f"historical_data_{symbol}.csv"
    if os.path.exists(filename):
        existing_data = pd.read_csv(filename, dtype=str)  # Read as strings
        last_date = existing_data['Date'].max()  # Get the max date as string
        new_row = pd.DataFrame([[date, close_price, volume]], columns=['Date', 'Close_Price', 'Volume'])
        
        # Append the new row without modifying existing data
        merged_data = pd.concat([existing_data, new_row]).drop_duplicates(subset=['Date']).sort_values(by='Date')
        merged_data.to_csv(filename, index=False)
    else:
        # If file doesn't exist, create a new one with the header
        with open(filename, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Date', 'Close_Price', 'Volume'])
            writer.writerow([date, close_price, volume])

# Go to the main page with the issuer symbols
url = 'https://www.mse.mk/en/stats/symbolhistory/TLE'
driver.get(url)

# Wait for the dropdown to be visible (instead of time.sleep)
wait = WebDriverWait(driver, 20)
dropdown = wait.until(EC.presence_of_element_located((By.ID, 'Code')))

# Scrape all the options from the dropdown
symbols = []
options = dropdown.find_elements(By.TAG_NAME, 'option')
for option in options:
    symbol = option.get_attribute('value')
    if symbol:
        symbols.append(symbol)

# Print out all the symbols found before filtering
print("All symbols:", symbols)

# Filter out symbols that contain numbers or any characters that are not letters or spaces
filtered_symbols = [symbol for symbol in symbols if re.match(r'^[A-Za-z\s]+$', symbol)]

# Print out the filtered symbols
print("Filtered symbols:", filtered_symbols)

# Get the date range for the past 10 years
start_date, end_date = get_date_range()

# Now, loop through each symbol and scrape the historical data page for it
for symbol in filtered_symbols:
    filename = f"historical_data_{symbol}.csv"
    last_date = None
    
    # Check if existing data file for symbol
    if os.path.exists(filename):
        existing_data = pd.read_csv(filename, dtype=str)  # Read as strings
        last_date = existing_data['Date'].max()
        
    start_date, end_date = get_date_range(last_date)
    
    # Build the URL dynamically for each symbol
    data_url = f'https://www.mse.mk/en/stats/symbolhistory/{symbol}'
    driver.get(data_url)
    
    # Wait for the "From Date" and "To Date" input fields to be present
    from_date_input = wait.until(EC.presence_of_element_located((By.ID, 'FromDate')))
    to_date_input = wait.until(EC.presence_of_element_located((By.ID, 'ToDate')))
    
    # Clear the current value and set the new date range
    from_date_input.clear()
    from_date_input.send_keys(start_date)
    to_date_input.clear()
    to_date_input.send_keys(end_date)
    
    # Wait for the "Submit" or "Search" button to be clickable before clicking it
    search_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'btn-primary-sm')))
    search_button.click()

    # Wait for the results table to appear after the page reloads
    results_table = wait.until(EC.presence_of_element_located((By.ID, 'resultsTable_wrapper')))
    
    # Get the page source after JavaScript has been executed
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # Find the table containing historical data
    table = soup.find('div', {'id': 'resultsTable_wrapper'})
    
    # Iterate over the rows and extract data
    if table:
        for row in table.find_all('tr')[1:]:  # Skip the header row
            columns = row.find_all('td')
            if len(columns) > 2:  # Ensure there are enough columns
                date = columns[0].text.strip()
                close_price = columns[1].text.strip()
                volume = columns[2].text.strip()
                update_historical_data(symbol, date, close_price, volume)
                print(f"{symbol} | {date} | {close_price} | {volume}")
    else:
        print(f"No historical data found for {symbol}")

def merge_csv_files(output_filename="combined_historical_data.csv"):
    all_files = glob.glob("historical_data_*.csv")  # Get all CSV files
    combined_data = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
    combined_data.to_csv(output_filename, index=False)
    print(f"All data has been merged into {output_filename}")

# Call the merge function after all data has been scraped and saved
merge_csv_files()

# Close the browser once done
driver.quit()
